{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_string(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./gpt/key.yaml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "client = OpenAI(api_key=config['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with curriculum and reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_curriculum():\n",
    "    initial_system = file_to_string('./curriculum_system.txt')\n",
    "    initial_user = file_to_string('./curriculum_user.txt')\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": initial_system},\n",
    "        {\"role\": \"user\", \"content\": initial_user}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Understanding the Basics of Balance\n",
      "\n",
      "Task 1 Description\n",
      "The agent should learn to maintain the hopper in an upright position with zero velocity (not falling over) for an extended period of time. The starting position is set with the hopper's torso standing vertically with a slight randomized initial disturbance in position and velocity. The reward is positively proportional to the time maintaining balance without exceeding a predefined maximum angle deviation from vertical.\n",
      "\n",
      "Task 2 Name\n",
      "Stationary Hopping\n",
      "\n",
      "Task 2 Description\n",
      "The agent must learn to perform a single hop and land back to the starting position without tilting or falling over. The hopper must remain in place (x-coordinate should not change significantly) while only moving in the z direction. The reward is given for successfully performing a hop and returning to the balance position within a small margin of error for x-position and angles of body parts. \n",
      "\n",
      "Task 3 Name\n",
      "Controlled Hopping\n",
      "\n",
      "Task 3 Description\n",
      "This task involves performing multiple hops in the same place, with the goal of achieving a rhythmic and consistent hopping pattern. The agent is tasked to control the applied torques to ensure that the hopper takes off and lands in the same x-coordinate while maintaining a healthy posture described by the is_healthy function. Rewards are provided for each successful hop and maintaining the upright position.\n",
      "\n",
      "Task 4 Name\n",
      "Directional Hopping\n",
      "\n",
      "Task 4 Description\n",
      "Now, the agent should learn to hop forward by applying torques that push the hopper in the right direction while still maintaining balance. The reward function should encourage forward motion with minimal sideways drift. The sequence of tasks should lead to developing an efficient gait that combines balance, power, and direction.\n",
      "\n",
      "Task 5 Name\n",
      "Efficient Forward Hopping\n",
      "\n",
      "Task 5 Description\n",
      "Here, the agent needs to optimize the hopping sequence for efficient forward travel. The agent must achieve higher forward velocity by improving the coordination of torques applied. The reward function should now heavily penalize inefficient energy usage and sideways movement, and provide bonuses for speed, as well as maintaining a healthy state as per the is_healthy function.\n",
      "\n",
      "Task 6 Name\n",
      "Maximize Distance within Time Limit\n",
      "\n",
      "Task 6 Description\n",
      "The goal for the agent here is to maximize the horizontal distance covered in a fixed time limit with a series of hops. The agent must apply the learnings from previous tasks to achieve the greatest distance possible before time runs out. The reward is based on the forward distance covered minus the control costs and the healthy state reward. There are penalties for falling over before the time limit.\n",
      "\n",
      "Task n Original Task\n",
      "Task n Original Task Description\n",
      "The agent should use the torques at its joints to achieve hops that move in the forward (right) direction while maximizing the cumulative reward. This involves complex coordination of all the learnings from previous tasks to maintain balance, control, and efficiency, minimizing control costs and maintaining a healthy torso angle and height throughout the hopping sequence. The performance of the hopper is evaluated based on the distance covered in the forward direction, subtracting the costs of control and ensuring a healthy state is maintained across the entire task.\n"
     ]
    }
   ],
   "source": [
    "curriculum_txt = generate_curriculum()\n",
    "\n",
    "with open('./curriculum.md', 'w') as file:\n",
    "    file.write(curriculum_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward():\n",
    "  reward_system = file_to_string('./reward_system.txt')\n",
    "  curriculum_user = file_to_string('./curriculum_user.txt')\n",
    "  reward_user = file_to_string('./curriculum.md')\n",
    "\n",
    "  user = curriculum_user + reward_user\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "     model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "     messages=[\n",
    "        {\"role\": \"system\", \"content\": reward_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "  \n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_functions = generate_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reflection():\n",
    "    reflection_system = file_to_string('./reflection_system.txt')\n",
    "    reflection_env_code = file_to_string('./reflection_user.txt')\n",
    "    task = file_to_string('./reflection_task.txt')\n",
    "    learning_curve = file_to_string('./reflection_learning_curve.txt')\n",
    "    reason = file_to_string('./reflection_reason.txt')\n",
    "\n",
    "    user = reflection_env_code + task + learning_curve + reason\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": reflection_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = generate_reflection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./reflection.md', 'w') as file:\n",
    "    file.write(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Trajectory Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback():\n",
    "  reflection_system = file_to_string('./trajectory_system.txt')\n",
    "  reflection_env_code = file_to_string('./trajectory_user.txt')\n",
    "  task = file_to_string('./trajectory_task.txt')\n",
    "  trajectory_1 = file_to_string('./simple_hopping_observation.txt')\n",
    "  trajectory_2 = file_to_string('./move_forward_observation.txt')\n",
    "\n",
    "  user = reflection_env_code + task + \"Trajectory of the agent 1: \\n\" + trajectory_1 + \"\\nTrajectory of the agent 2: \\n\" + trajectory_2\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": reflection_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 2\n",
      "Reason: The task specifies that the robot should maintain vertical jumping and landing without progressing in the x-direction. Upon examining the trajectories for both agents, it is evident that agent 1 has x positions that vary from 0.00 to 1.00 and keep increasing, which indicates horizontal movement. In contrast, agent 2 shows the x position consistently being 0.00 or very close to it across all timesteps, indicating successful adherence to the task of not moving horizontally while performing vertical jumps. Agent 2's trajectory is better aligned with the task description as it minimizes horizontal displacement.\n"
     ]
    }
   ],
   "source": [
    "decision = feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_feedback():\n",
    "  reflection_system = file_to_string('./trajectory_system.txt')\n",
    "  reflection_env_code = file_to_string('./trajectory_user.txt')\n",
    "  task = file_to_string('./trajectory_task.txt')\n",
    "  trajectory_1 = file_to_string('./stand_still_observation.txt')\n",
    "\n",
    "  user = reflection_env_code + task + \"Trajectory of the agent: \\n\" + trajectory_1\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": reflection_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Not following\n",
      "Reason: The initial z-coordinate of the torso is 1.25 and falls within the healthy range. However, the robot's z-coordinate starts to dip below 1.25 partway through the trajectory, dropping down to 1.20 and eventually to 1.21, which is outside the specified healthy range of z-coordinate. Therefore, the robot is not correctly maintaining the height parameter of the task.\n"
     ]
    }
   ],
   "source": [
    "decision = single_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward():\n",
    "  reward_system = file_to_string('./reward_system.txt')\n",
    "  reward_user = file_to_string('./reward_user.txt')\n",
    "  task = file_to_string('./trajectory_task.txt')\n",
    "\n",
    "  user = reward_user + task\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "     model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "     messages=[\n",
    "        {\"role\": \"system\", \"content\": reward_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "  \n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To devise reward functions for Task 2 where the goal is to maintain vertical jumping and landing without progressing in the x-direction, we need to utilize the given observation details. Specifically, the reward functions will focus on the z height of the torso (`observation[1]`), the change in x position (`observation[0]` before and `next_observation[0]` after), as well as the torques applied as actions and whether the hopper is in a healthy state.\n",
      "\n",
      "Sample 1\n",
      "```python\n",
      "from typing import List, Tuple, Dict\n",
      "import numpy as np\n",
      "\n",
      "def compute_reward_1(observation: List, action: List, next_observation: List) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_height = next_observation[1]\n",
      "    change_in_x = abs(next_observation[0] - observation[0])\n",
      "    healthy = 1 if next_observation[1] >= 0.7 else 0  # Using 0.7 as the threshold for healthy z height\n",
      "    \n",
      "    # Reward Components\n",
      "    vertical_reward = np.clip(z_height, 0, 2)  # Encouraging vertical movement\n",
      "    horizontal_penalty = -change_in_x  # Discouraging horizontal movement\n",
      "    health_reward = 100 * healthy  # Big reward for being in a healthy state\n",
      "\n",
      "    # Summing up components for the total reward\n",
      "    reward = vertical_reward + horizontal_penalty + health_reward\n",
      "    \n",
      "    # Reward Components Dictionary\n",
      "    reward_components = {\n",
      "        'vertical_reward': vertical_reward,\n",
      "        'horizontal_penalty': horizontal_penalty,\n",
      "        'health_reward': health_reward\n",
      "    }\n",
      "    \n",
      "    return np.float64(reward), reward_components\n",
      "```\n",
      "\n",
      "Sample 2\n",
      "```python\n",
      "def compute_reward_2(observation: List, action: List, next_observation: List) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_velocity = (next_observation[1] - observation[1]) / 0.001  # Assuming time step of 0.001\n",
      "    horizontal_penalty = -np.square(observation[0] - next_observation[0])  # Square penalty for horizontal movement\n",
      "    \n",
      "    healthy = 1 if next_observation[1] >= 0.8 else 0  # threshold for healthy z height\n",
      "    health_reward = 50 * healthy  # Modest reward for being in a healthy state\n",
      "\n",
      "    # Summing up components for the total reward\n",
      "    reward = z_velocity + horizontal_penalty + health_reward\n",
      "    \n",
      "    # Reward Components Dictionary\n",
      "    reward_components = {\n",
      "        'z_velocity': z_velocity,\n",
      "        'horizontal_penalty': horizontal_penalty,\n",
      "        'health_reward': health_reward\n",
      "    }\n",
      "    \n",
      "    return np.float64(reward), reward_components\n",
      "```\n",
      "\n",
      "Sample 3\n",
      "```python\n",
      "def compute_reward_3(observation: List, action: List, next_observation: List) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_height_reward = np.log1p(next_observation[1])  # Logarithmic reward for height to encourage gentle landings\n",
      "    horizontal_penalty = -np.abs(observation[0])  # Linear penalty for horizontal position\n",
      "    \n",
      "    healthy = 1 if observation[2] >= -0.2 and observation[2] <= 0.2 else 0  # threshold for healthy angle\n",
      "    angle_health_reward = 20 * healthy  # Reward for maintaining a healthy angle\n",
      "    \n",
      "    # Reward based on the use of minimal control\n",
      "    control_cost = -np.sum(np.square(action))\n",
      "    \n",
      "    # Summing up components for the total reward\n",
      "    reward = z_height_reward + horizontal_penalty + angle_health_reward + control_cost\n",
      "    \n",
      "    # Reward Components Dictionary\n",
      "    reward_components = {\n",
      "        'z_height_reward': z_height_reward,\n",
      "        'horizontal_penalty': horizontal_penalty,\n",
      "        'angle_health_reward': angle_health_reward,\n",
      "        'control_cost': control_cost\n",
      "    }\n",
      "\n",
      "    return np.float64(reward), reward_components\n",
      "```\n",
      "\n",
      "Sample 4\n",
      "```python\n",
      "import math\n",
      "\n",
      "def compute_reward_4(observation: List, action: List, next_observation: List) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_change = next_observation[1] - observation[1]  # Change in z\n",
      "    x_stability_penalty = -math.exp(abs(observation[0]))  # Exponential penalty for x deviation\n",
      "    \n",
      "    # Check if hopper's torso is upright\n",
      "    upright_penalty = -abs(next_observation[2])  # Penalty for leaning too much\n",
      "    \n",
      "    # Reward for being healthy is binary based on z height range\n",
      "    healthy = 1 if next_observation[1] > 0.6 else 0  # A bit lenient in z height\n",
      "    health_reward = 50 * healthy\n",
      "\n",
      "    # Summing up components for the total reward\n",
      "    reward = z_change + x_stability_penalty + upright_penalty + health_reward\n",
      "    \n",
      "    # Reward Components Dictionary\n",
      "    reward_components = {\n",
      "        'z_change': z_change,\n",
      "        'x_stability_penalty': x_stability_penalty,\n",
      "        'upright_penalty': upright_penalty,\n",
      "        'health_reward': health_reward\n",
      "    }\n",
      "\n",
      "    return np.float64(reward), reward_components\n",
      "```\n",
      "\n",
      "Sample 5\n",
      "```python\n",
      "def compute_reward_5(observation: List, action: List, next_observation: List) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_velocity_bonus = np.sign(next_observation[1] - observation[1]) * np.sqrt(abs(next_observation[1] - observation[1]))\n",
      "    horizontal_movement_penalty = -pow(abs(observation[0] - next_observation[0]), 2)\n",
      "    \n",
      "    # Rewarding for being healthy with a multiplicative factor to other rewards\n",
      "    healthy = next_observation[1] > 0.7 and abs(next_observation[2]) < 0.2\n",
      "    health_bonus = 10 * healthy\n",
      "\n",
      "    # Punishment for large control signals (action torques)\n",
      "    control_cost_penalty = -np.sum(np.square(action)) * 0.01\n",
      "\n",
      "    # Summing up components for the total reward\n",
      "    reward = z_velocity_bonus + horizontal_movement_penalty + health_bonus + control_cost_penalty\n",
      "    \n",
      "    # Reward Components Dictionary\n",
      "    reward_components = {\n",
      "        'z_velocity_bonus': z_velocity_bonus,\n",
      "        'horizontal_movement_penalty': horizontal_movement_penalty,\n",
      "        'health_bonus': health_bonus,\n",
      "        'control_cost_penalty': control_cost_penalty\n",
      "    }\n",
      "\n",
      "    return np.float64(reward), reward_components\n",
      "```\n",
      "\n",
      "These are five sample reward functions designed to promote vertical movement while penalizing horizontal progression and maintaining the hopper's health and stability. Each sample uses a different set of variables and weights for the components to create a well-functioning reward function specific to the given reinforcement learning task.\n"
     ]
    }
   ],
   "source": [
    "rewards = generate_reward()\n",
    "with open('./rewards.md', 'w') as file:\n",
    "    file.write(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rewards():\n",
    "  reward_system = file_to_string('./compare_reward_system.txt')\n",
    "  reward_user = file_to_string('./reward_user.txt')\n",
    "  task = file_to_string('./trajectory_task.txt')\n",
    "  rewards = file_to_string('./rewards.md')\n",
    "\n",
    "  user = reward_user + task + rewards\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "     model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "     messages=[\n",
    "        {\"role\": \"system\", \"content\": reward_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "  \n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Sample 4\n",
      "\n",
      "Reason:\n",
      "Looking at the task description for simple hopping, the agent's goal is to maintain vertical jumping and landing without moving in the x-direction. Sample 4's reward function aligns well with the task at hand.\n",
      "\n",
      "Let's break down the reasons according to the reward function criteria:\n",
      "\n",
      "1. Alignment with task description: In Sample 4, the reward function provides a binary reward for maintaining the healthy z-height between 0.7 and 2.0 and torso angle within the acceptable range of -0.2 to 0.2. The conditions align with the environment's `is_healthy` method and encourage the behavior of staying in place while hopping. The stillness and x-movement penalty that should be present according to the task description is implicitly addressed by rewarding only the correct z-height range, which can be achieved by vertical movement.\n",
      "\n",
      "2. Simplicity: Sample 4 offers the simplest reward structure among the presented samples, which is a combination of the upright posture reward and the action cost. The simplicity means it may be easier for the agent to understand what the reward function requires and to learn appropriate behaviors.\n",
      "\n",
      "3. Reward output positivity: Sample 4 handles reward positivity by giving binary rewards; the action cost is negative due to the action taken, which is consistent across all samples and encourages energy-efficient control.\n",
      "\n",
      "4. Reward scale: Sample 4 doesn't specify large magnitudes for rewards and uses a straightforward action cost scaling of 0.1 consistent with the given `control_cost` function within the environment code, which seems reasonable and not too large or too small.\n",
      "\n",
      "The other samples either introduce unnecessary complexity (Sample 5 with exponential functions), don't properly discourage x-movement (Sample 1), or assign large rewards for maintaining a healthy state (Sample 1), which might overshadow the hopping objective. Sample 3's use of a logarithmic function for z-height might overly complicate the agent's understanding of the desired behavior. Sample 2â€™s stillness penalty has a small temperature, and the use of the exponential function may result in less intuitive gradients for learning.\n"
     ]
    }
   ],
   "source": [
    "decision = compare_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_reward():\n",
    "  reward_system = file_to_string('./reward_system.txt')\n",
    "  env_user = file_to_string('./chain_user.txt')\n",
    "  reward_user = file_to_string('./curriculum.md')\n",
    "\n",
    "  user = env_user + reward_user\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "     model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "     messages=[\n",
    "        {\"role\": \"system\", \"content\": reward_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "  \n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_reward = chain_reward()\n",
    "with open('./chain_reward.md', 'w') as file:\n",
    "    file.write(chain_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Function generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success():\n",
    "    system = file_to_string('./success_system.txt')\n",
    "    user = file_to_string('./success_user.txt')\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided environment code and the description of Task 2, the success of the \"Learn to Squat and Rise\" task can be evaluated using the z height of the torso, the angles of the joints, and the overall health of the state as described in the `is_healthy` method. We need to ensure that the Hopper stays within healthy ranges without hopping or moving horizontally. \n",
      "\n",
      "The success function will check that:\n",
      "1. The next z height is within the specified healthy range.\n",
      "2. The hopper should not be hopping, which implies there should be minimal change in the x component of position between observations.\n",
      "3. The angles of the thigh, leg, and foot joints remain within healthy boundaries.\n",
      "4. The hopper stays 'healthy' according to the `is_healthy` function.\n",
      "\n",
      "```python\n",
      "def compute_success(observation, action, next_observation) -> bool:\n",
      "    # Extract the relevant parts of the observation\n",
      "    prev_z, next_z = observation[1], next_observation[1]\n",
      "    prev_x, next_x = observation[0], next_observation[0]\n",
      "    angles = next_observation[3:6]  # thigh, leg, foot joint angles\n",
      "\n",
      "    # Define the healthy bounds as provided in the environment code\n",
      "    min_z, max_z = (0.7, np.inf)\n",
      "    min_angle, max_angle = (-0.2, 0.2)\n",
      "\n",
      "    # Check if the hopper's z height is within the healthy range\n",
      "    success_z = min_z < next_z < max_z\n",
      "    \n",
      "    # Check if the hopper has not moved horizontally significantly which implies no hopping\n",
      "    success_x = (abs(next_x - prev_x) < 0.01)  # Allow for a small tolerance in horizontal movement\n",
      "    \n",
      "    # Check if the joint angles are within the healthy range\n",
      "    success_angles = all(min_angle < angle < max_angle for angle in angles)\n",
      "    \n",
      "    # Check if the overall state is healthy\n",
      "    success_health = next_observation[7]  # 7th index in next_observation contains the health status\n",
      "    \n",
      "    # Success is when all conditions are true\n",
      "    success = success_z and success_x and success_angles and bool(success_health)\n",
      "    \n",
      "    return success\n",
      "```\n",
      "\n",
      "This function assumes that `next_observation` already contains the result of calling `is_healthy` method as the 7th index, which is a Boolean value indicating the healthy state of the agent. If the `next_observation` does not include the result of `is_healthy`, that part of the code should be adjusted accordingly.\n"
     ]
    }
   ],
   "source": [
    "success_function = success()\n",
    "with open('./success.md', 'w') as file:\n",
    "    file.write(success_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment specific feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_feedback(x_pos_av, x_pos_std, z_pos_av, z_pos_std, x_vel_av, x_vel_std, z_vel_av, z_vel_std):\n",
    "  reflection_system = file_to_string('./trajectory_system.txt')\n",
    "  reflection_env_code = file_to_string('./trajectory_user.txt')\n",
    "  task = file_to_string('./trajectory_task.txt')\n",
    "  trajectory_1 = file_to_string('./stand_still_observation.txt')\n",
    "\n",
    "  user = reflection_env_code + task + \"Trajectory of the agent: \\n\" + trajectory_1 \n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": reflection_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./stand_still_observation.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "stand_still_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./simple_hopping_observation.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "simple_hopping_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./move_forward_observation.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "move_forward_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback(traj_1_info, traj_2_info):\n",
    "  reflection_system = file_to_string('./trajectory_system.txt')\n",
    "  reflection_env_code = file_to_string('./trajectory_user.txt')\n",
    "  task = file_to_string('./trajectory_task.txt')\n",
    "\n",
    "  user = reflection_env_code + task + \"Trajectory information of the agent 1: \\n\" + traj_1_info + \"\\nTrajectory information of the agent 2: \\n\" + traj_2_info\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": reflection_system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  print(completion.choices[0].message.content)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 2\n",
      "Reason: Agent 2 better maintains its position in the x-direction, as both its average x position and standard deviation of x position are zero, indicating that it does not move horizontally. This is in line with the task description, which specifies that the agent's goal is to maintain vertical jumping and landing without progressing in the x-direction. The average z position is reasonably high, which indicates hopping. Standard deviation for z position is very small, showing consistent hopping height. Moreover, the average x velocity is zero with no standard deviation, which suggests that there is no horizontal movement over time, and a small standard deviation of z velocity indicates controlled vertical hopping. Agent 1 shows some horizontal deviation and hence, does not strictly follow the task description compared to Agent 2.\n"
     ]
    }
   ],
   "source": [
    "feedback_txt = feedback(simple_hopping_info, stand_still_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average x position: 0.0\n",
      "Standard deviation of x position: 0.0\n",
      "Average z position: 1.2101798201798204\n",
      "Standard deviation of z position: 0.0033100869613864286\n",
      "Average x velocity: 0.0\n",
      "Standard deviation of x velocity: 0.0\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 0.18540496217739175\n",
      "\n",
      "Average x position: 0.21748251748251748\n",
      "Standard deviation of x position: 0.05654607400669276\n",
      "Average z position: 1.4222977022977024\n",
      "Standard deviation of z position: 0.16082117967756113\n",
      "Average x velocity: -7.105427357601002e-18\n",
      "Standard deviation of x velocity: 0.9826622003516774\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 1.4437364717980912\n",
      "\n",
      "Average x position: 1.0750218340611355\n",
      "Standard deviation of x position: 0.9117063154529592\n",
      "Average z position: 1.3292576419213973\n",
      "Standard deviation of z position: 0.16504033910927382\n",
      "Average x velocity: 1.6995614035087723\n",
      "Standard deviation of x velocity: 1.124478351067322\n",
      "Average z velocity: -0.2905701754385964\n",
      "Standard deviation of z velocity: 1.4354720262192031\n"
     ]
    }
   ],
   "source": [
    "print(stand_still_info)\n",
    "print(simple_hopping_info)\n",
    "print(move_forward_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
