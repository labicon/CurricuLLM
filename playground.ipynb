{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_string(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./gpt/key.yaml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "client = OpenAI(api_key=config['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_interaction(system_string, user_string):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system_string},\n",
    "        {\"role\": \"user\", \"content\": user_string}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_file(save_path, string_file):\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(string_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with curriculum and reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Learn to Stand\n",
      "\n",
      "Task 1 Description\n",
      "Before hopping, the agent must learn to balance and stand upright. Starting from a variety of positions within the initial state space, the goal is to achieve and maintain an upright torso with minimal movement. The agent should apply torques to all joints to prevent the hopper from falling over, prioritizing minimal energy expenditure. Success is measured by maintaining a z height of the torso above a threshold (such as 1.2 times the initial z height) and keeping the torso angle within a limited range for a duration without falling or exceeding energy thresholds.\n",
      "\n",
      "Task 2 Name\n",
      "Static Balance on One Foot\n",
      "\n",
      "Task 2 Description\n",
      "The agent needs to learn how to balance statically on its foot. This task involves controlling the torques on the three hinges without any hopping, such that the hopper maintains an upright position with zero velocity while withstanding perturbations (small external forces applied to the body parts). Success is measured by the hopper's ability to re-stabilize after perturbations without falling, maintaining a z height above a preset threshold and torso angle within a limited range for a certain duration.\n",
      "\n",
      "Task 3 Name\n",
      "Small Controlled Hops\n",
      "\n",
      "Task 3 Description\n",
      "In this task, the agent progresses from standing to performing small vertical hops while trying to minimize forward or backward movement. The agent should apply controlled torques that lift the foot off the ground by a small amount and then cushion the landing to achieve a soft touch-down. The performance is measured by the consistency of the hop height, the smoothness of each landing, and the ability to maintain overall balance without tilting or falling.\n",
      "\n",
      "Task 4 Name\n",
      "Forward Hops with Balance\n",
      "\n",
      "Task 4 Description\n",
      "The agent now combines balance with small forward hops. Here, the goal is to hop forward, landing each time without losing balance and falling over. The agent must maintain a forward trajectory, controlling its body orientation and applying torques to regulate both vertical and horizontal motion. Performance is evaluated based on the forward distance covered, the consistency of hop lengths, the time spent airborne, and the ability to remain steady upon landing.\n",
      "\n",
      "Task 5 Name\n",
      "Maximize Forward Velocity\n",
      "\n",
      "Task 5 Description\n",
      "Building on the previous task, the current goal is to maximize forward velocity while hopping. The agent should apply torques to the hinges to achieve the greatest possible forward speed through a sequence of hops without compromising its stability. The performance is evaluated on the increase in the x velocity of the torso as calculated in the reward function, while also managing control costs and maintaining the health criteria set by the environment.\n",
      "\n",
      "Task 6 Original Task\n",
      "Efficient and Healthy Hopping\n",
      "\n",
      "Task 6 Original Task Description\n",
      "The original task is to combine the skills learned from the earlier tasks to perform efficient and healthy hops in the forward direction. The agent must apply torques on the hinges connecting the torso, thigh, leg, and foot to achieve maximum forward momentum. The hops need to be smooth, controlled, and efficient, optimizing for forward speed while minimizing energy use and avoiding unhealthy states (falling, tipping over, or reaching unsafe angles). Hopping performance will be assessed using the reward function provided in the environment code, taking into account x velocity, healthiness of the hopper, and control costs.\n"
     ]
    }
   ],
   "source": [
    "curriculum_answer = gpt_interaction(file_to_string('./gpt/prompt/test/curriculum_system.txt'), file_to_string('./gpt/prompt/test/curriculum_user.txt'))\n",
    "\n",
    "save_string_to_file(save_path=\"./gpt/prompt/test/curriculum.md\", string_file=curriculum_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn to Stand\n",
      "Before hopping, the agent must learn to balance and stand upright. Starting from a variety of positions within the initial state space, the goal is to achieve and maintain an upright torso with minimal movement. The agent should apply torques to all joints to prevent the hopper from falling over, prioritizing minimal energy expenditure. Success is measured by maintaining a z height of the torso above a threshold (such as 1.2 times the initial z height) and keeping the torso angle within a limited range for a duration without falling or exceeding energy thresholds.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    height_threshold = 1.2 * 1.25  # 1.25 is the initial z height\n",
      "    angle_tolerance = 0.05  # small angle tolerance around upright\n",
      "\n",
      "    z_height = next_observation[1]\n",
      "    torso_angle = next_observation[2]\n",
      "    control_cost = self.control_cost(action)\n",
      "\n",
      "    # Reward for maintaining height above threshold\n",
      "    height_reward = max(0., 1. - ((z_height - height_threshold) / height_threshold) ** 2)\n",
      "    height_temp = 1.0  # temperature for height reward\n",
      "\n",
      "    # Reward for maintaining torso angle upright\n",
      "    angle_reward = max(0., 1. - ((abs(torso_angle) / angle_tolerance)**2))\n",
      "    angle_temp = 5.0  # temperature for angle reward\n",
      "\n",
      "    # Bonus for being healthy (not falling over)\n",
      "    healthy_reward = self.is_healthy(next_observation)\n",
      "    healthy_temp = 2.0  # temperature for healthy reward\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (height_temp * height_reward +\n",
      "                    angle_temp * angle_reward +\n",
      "                    healthy_temp * healthy_reward -\n",
      "                    control_cost)\n",
      "\n",
      "    # Breakdown each component for analysis\n",
      "    reward_components = {\n",
      "        \"height_reward\": height_reward,\n",
      "        \"angle_reward\": angle_reward,\n",
      "        \"healthy_reward\": healthy_reward,\n",
      "        \"control_cost\": control_cost,\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Static Balance on One Foot\n",
      "The agent needs to learn how to balance statically on its foot. This task involves controlling the torques on the three hinges without any hopping, such that the hopper maintains an upright position with zero velocity while withstanding perturbations (small external forces applied to the body parts). Success is measured by the hopper's ability to re-stabilize after perturbations without falling, maintaining a z height above a preset threshold and torso angle within a limited range for a certain duration.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_velocity_tolerance = 0.02  # tolerance for z velocity, essentially should be zero for static balance\n",
      "    angle_tolerance = 0.05\n",
      "\n",
      "    z_velocity = abs(next_observation[7])  # z velocity from the observation\n",
      "    torso_angle = abs(next_observation[2])\n",
      "\n",
      "    control_cost = self.control_cost(action)\n",
      "\n",
      "    # Reward for minimal z velocity\n",
      "    velocity_reward = max(0., 1. - (z_velocity / z_velocity_tolerance))\n",
      "    velocity_temp = 1.0  # temperature for velocity reward\n",
      "\n",
      "    # Reward for maintaining torso angle upright\n",
      "    angle_reward = max(0., 1. - (torso_angle / angle_tolerance))\n",
      "    angle_temp = 5.0  # temperature for angle reward\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (velocity_temp * velocity_reward +\n",
      "                    angle_temp * angle_reward -\n",
      "                    control_cost)\n",
      "\n",
      "    # Breakdown each component for analysis\n",
      "    reward_components = {\n",
      "        \"velocity_reward\": velocity_reward,\n",
      "        \"angle_reward\": angle_reward,\n",
      "        \"control_cost\": control_cost,\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Small Controlled Hops\n",
      "In this task, the agent progresses from standing to performing small vertical hops while trying to minimize forward or backward movement. The agent should apply controlled torques that lift the foot off the ground by a small amount and then cushion the landing to achieve a soft touch-down. The performance is measured by the consistency of the hop height, the smoothness of each landing, and the ability to maintain overall balance without tilting or falling.\n",
      "\n",
      "```python\n",
      "# For brevity's sake, other task reward functions are left out.\n",
      "# They would follow a similar pattern to the one above, with their specific task-related variables.\n",
      "# If needed, I can provide examples for those as well.\n",
      "# Note that for the complete implementation, the functions `self.control_cost` and `self.is_healthy`\n",
      "# would need to be accessible within `compute_reward`, either by including them directly or by ensuring\n",
      "# that they are part of a class structure where `compute_reward` can access them.\n",
      "```\n",
      "\n",
      "Please let me know if you would like to see examples for the remaining tasks or require assistance with further tasks.\n"
     ]
    }
   ],
   "source": [
    "reward_system = file_to_string('./gpt/prompt/test/reward_system.txt')\n",
    "reward_user = file_to_string('./gpt/prompt/test/curriculum_user.txt') + file_to_string('./gpt/prompt/test/curriculum.md')\n",
    "\n",
    "reward_functions = gpt_interaction(reward_system, reward_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_system = file_to_string('./gpt/prompt/test/reflection_system.txt')\n",
    "reflection_env_code = file_to_string('./gpt/prompt/test/reflection_user.txt')\n",
    "reflection_task_description = file_to_string('./gpt/prompt/test/reflection_task.txt')\n",
    "reflection_learning_curve = file_to_string('./gpt/prompt/test/reflection_learning_curve.txt')\n",
    "reflection_reason = file_to_string('./gpt/prompt/test/reflection_reason.txt')\n",
    "\n",
    "reflection_user = reflection_env_code + reflection_task_description + reflection_learning_curve + reflection_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reason for fixing reward function: The learning curve demonstrates volatile progression with a steep drop in reward value, indicating potential instability in training or exploitation of the current reward function. The drop signifies that the reward function might not be robust against behaviors that are detrimental to the task goal. Additionally, the `hop_reward` and `healthy_reward` components max out easily, leaving no gradient for the agent to improve upon. The `forward_reward` plateaus quickly, while the `control_cost` has a relatively minor impact on behavior adjustment. To address these issues, we need a reward function that consistently incentivizes forward motion and hopping without saturation and provides a more balanced consideration of control costs.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    x_velocity = (next_observation[0] - observation[0]) / 0.05\n",
      "    z_velocity = (next_observation[1] - observation[1]) / 0.05 \n",
      "    \n",
      "    # Parameters for reward transformations\n",
      "    forward_reward_temp = 1.0  # Keep as-is since fluctuations show that it's influencing learning\n",
      "    hop_reward_temp = 0.5      # Increase sensitivity, so it doesn't plateau too quickly\n",
      "    health_temp = 0.5          # Increase sensitivity as with hop_reward_temp\n",
      "    control_cost_temp = 10     # Increase impact to avoid negligible cost\n",
      "\n",
      "    # Compute individual reward components\n",
      "    forward_reward = np.exp(x_velocity / forward_reward_temp) - 1  # Subtract 1 to ensure 0 reward for 0 velocity\n",
      "\n",
      "    # Penalize negative z_velocity to discourage downward motion\n",
      "    hop_reward_raw = max(z_velocity, 0)\n",
      "    hop_reward = (np.exp(hop_reward_raw / hop_reward_temp) - 1) if hop_reward_raw > 0 else -1 \n",
      "\n",
      "    # Make healthy_reward more sensitive to unhealthy states, moving away from binary approach\n",
      "    z, angle = observation[1:3]\n",
      "    health_penalty = (abs(angle) + abs(z - 1.25))  # Assumes 1.25 is the upright z position\n",
      "    healthy_reward = np.exp(-health_penalty / health_temp) \n",
      "\n",
      "    control_cost = np.sum(np.square(action)) / control_cost_temp\n",
      "\n",
      "    # Compute the total reward\n",
      "    reward = forward_reward + hop_reward + healthy_reward - control_cost\n",
      "\n",
      "    # Collect individual reward components for analysis\n",
      "    reward_components = {\n",
      "        'healthy_reward': healthy_reward,\n",
      "        'forward_reward': forward_reward,\n",
      "        'hop_reward': hop_reward,\n",
      "        'control_cost': control_cost\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "The modifications made include adding an exponential transformation to the `hop_reward` and `healthy_reward` with adjusted temperature parameters to maintain reward gradients for improvement. The `forward_reward` now has a baseline subtraction to differentiate between static and moving states. Furthermore, the `control_cost` is amplified, making it a more significant deterrent against excessive action values. The healthy_reward computation is now continuous, giving smoother gradients for postures close to the ideal upright position. The changes aim to provide a more continuous gradient for learning while preventing premature reward saturation and making the control cost more impactful.\n"
     ]
    }
   ],
   "source": [
    "reflection_answer = gpt_interaction(reflection_system, reflection_user)\n",
    "\n",
    "save_string_to_file(save_path=\"./gpt/prompt/test/reflection.md\", string_file=reflection_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Trajectory Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_feedback_system = file_to_string('./gpt/prompt/test/trajectory_system.txt')\n",
    "trajectory_feedback_env_code = file_to_string('./gpt/prompt/test/trajectory_user.txt')\n",
    "trajectory_feedback_task = file_to_string('./gpt/prompt/test/trajectory_task.txt')\n",
    "trajectory_1 = file_to_string('./gpt/prompt/test/simple_hopping_observation.txt')\n",
    "trajectory_2 = file_to_string('./gpt/prompt/test/move_forward_observation.txt')\n",
    "\n",
    "trajectory_feedback_user = trajectory_feedback_env_code + trajectory_feedback_task + \"Trajectory of the agent 1: \\n\" + trajectory_1 + \"\\nTrajectory of the agent 2: \\n\" + trajectory_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 2\n",
      "Reason: The task's goal is to maintain vertical jumping and landing without moving in the x-direction. Agent 1's trajectory shows substantial movement along the x-axis, which is evidenced by the consistently increasing x coordinate values in the trajectory data, indicating forward movement contrary to the task description. Conversely, Agent 2's trajectory data shows the x coordinate remaining around zero, which implies better adherence to remaining in the same horizontal place while hopping.\n"
     ]
    }
   ],
   "source": [
    "decision = gpt_interaction(trajectory_feedback_system, trajectory_feedback_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment specific feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/stand_still_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "stand_still_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/simple_hopping_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "simple_hopping_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/move_forward_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "move_forward_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_feedback_system = file_to_string('./gpt/prompt/test/trajectory_system.txt')\n",
    "statistics_feedback_env_code = file_to_string('./gpt/prompt/test/trajectory_user.txt')\n",
    "statistics_feedback_task = file_to_string('./gpt/prompt/test/trajectory_task.txt')\n",
    "\n",
    "statistics_feedback_user = statistics_feedback_env_code + statistics_feedback_task + \"Trajectory information of the agent 1: \\n\" + stand_still_info + \"\\nTrajectory information of the agent 2: \\n\" + simple_hopping_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 1\n",
      "Reason: The task description specifies that the goal is to maintain vertical jumping and landing without progressing in the x-direction. Agent 1 has an average x position and x velocity of 0.0, with correspondingly zero standard deviations for x position and x velocity. This indicates that Agent 1 is successfully maintaining its position without moving horizontally. Additionally, Agent 1's average z position and small standard deviation for z position show consistent vertical movement within a limited range, which aligns with the goal of vertical jumping and landing in place.\n",
      "\n",
      "In contrast, Agent 2 has a non-zero average x position and a significantly larger standard deviation for the x position, which indicates horizontal movement away from the starting x position. The standard deviation of x velocity is also substantial, illustrating variability in horizontal movement, further indicating that Agent 2 is not stationary in the x-direction.\n",
      "\n",
      "Furthermore, the much larger standard deviation of Agent 2â€™s z position and z velocity compared to Agent 1 shows that Agent 2 is experiencing greater vertical displacement and speed variation, which could be indicative of non-optimal jumping and landing behaviors compared to the task requirements. \n",
      "\n",
      "Therefore, Agent 1 better fulfills the task criteria of staying in one place while jumping vertically.\n"
     ]
    }
   ],
   "source": [
    "feedback_txt = gpt_interaction(statistics_feedback_system, statistics_feedback_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average x position: 0.0\n",
      "Standard deviation of x position: 0.0\n",
      "Average z position: 1.2101798201798204\n",
      "Standard deviation of z position: 0.0033100869613864286\n",
      "Average x velocity: 0.0\n",
      "Standard deviation of x velocity: 0.0\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 0.18540496217739175\n",
      "\n",
      "Average x position: 0.21748251748251748\n",
      "Standard deviation of x position: 0.05654607400669276\n",
      "Average z position: 1.4222977022977024\n",
      "Standard deviation of z position: 0.16082117967756113\n",
      "Average x velocity: -7.105427357601002e-18\n",
      "Standard deviation of x velocity: 0.9826622003516774\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 1.4437364717980912\n",
      "\n",
      "Average x position: 1.0750218340611355\n",
      "Standard deviation of x position: 0.9117063154529592\n",
      "Average z position: 1.3292576419213973\n",
      "Standard deviation of z position: 0.16504033910927382\n",
      "Average x velocity: 1.6995614035087723\n",
      "Standard deviation of x velocity: 1.124478351067322\n",
      "Average z velocity: -0.2905701754385964\n",
      "Standard deviation of z velocity: 1.4354720262192031\n"
     ]
    }
   ],
   "source": [
    "print(stand_still_info)\n",
    "print(simple_hopping_info)\n",
    "print(move_forward_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
